# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction_using_Machine_Learning_Techniques.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YkLhK20Sjn9PWGQx_5rAdrdhV6OXNKU1

<center>
    <h1><b>Heart Disease Prediction using Machine Learning Techniques</b></h1>
</center>

<p><b>Name:</b> Aashish Bansal</p>
<p><b>Registration No.:</b> 19BIT0346</p>
<p><b>Department:</b> Information Technology</p>
<p><b>School:</b> School of Information Technology and Engineering</p>

# Connecting to Data Source (Google Drive)
"""

from google.colab import drive
drive.mount('/content/mydrive')

"""# Installing Libraries"""

!pip install pyforest

"""# Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
from pyforest import *
lazy_imports()
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline

"""# Loading the Dataset

## About the Dataset

Source of Dataset : https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset
"""

#load dataset
df = pd.read_csv("https://raw.githubusercontent.com/aashish22bansal/Heart-Disease-Prediction/main/data/framingham.csv")

"""# Exploratory Data Analysis

## Shape of the Dataset
"""

# Check number of columns and rows in data frame
#Shape of dataset

print ('Number of Examples :', df.shape[0], '\nNumber of Features : ', df.shape[1])

df.head()

"""## Basic Information from the Dataset"""

df.info()

"""Above results shows us that we've :
* (7) x int64 datatype attributes
* (9) x float64 datatype attributes.

## Basic Statistical Deductions
"""

df.describe()

"""Descriptive Statistics :

* We can observe from above table that we have 123 examples.
* The Spread of Age is from 32 to 74.
* Resting blood pressure is distributed from 80 to 200 (in mm Hg).
* Similarly, thalach (maximum heart rate achieved) ranges from 60 to 182.
* While, oldpeak (ST depression induced by exercise relative to rest) values ranges from -2.60 to 3.70.

## Columns in the Dataset
"""

df.columns

"""# Data Visualization

## Plotting Data from all Columns
"""

#get familiar with the dataset 
df.hist(bins=30, 
        figsize=(20,40),
        layout=(15,4));

"""## Age Vs Heart Disease"""

# age vs CHD
plt.figure(figsize=(10,10))
sns.swarmplot(x='TenYearCHD', y='age', data=df)

plt.figure(figsize=(10,10))
sns.violinplot(x='TenYearCHD', y='age', data=df)

"""Observations:
* Violinplot tells that most patients of age around 40-55 have 0 risk
* Most patients of age around 60-65 have risk of disease (CHD)

## Age vs Heart Disease for Smokers and Non-Smokers
"""

# age vs CHD for smokers or non-smoker
plt.figure(figsize=(10,10))
sns.swarmplot(x='TenYearCHD', y='age', data=df, hue='currentSmoker')

plt.figure(figsize=(10,10))
sns.violinplot(x='TenYearCHD', y='age', data=df, hue='currentSmoker', split=True)

"""Observations:
* From this violinplot, we see that most of smokers having no risk of CHD are in age around 40 years
* But most of non-smokers having risk are in age around 65-70 years Also most smokers having risk are in age around 50 years

## Gender Count
"""

# male and female countplot
sns.countplot(x=df['male'])

"""## Gender vs Heart Disease"""

# male and female having disease or not
sns.countplot(x=df['male'], hue=df['TenYearCHD'])

"""Observations:
* Here from the above countplot, we see that most data are females
* There are more females having no risk than males having no risk
* There are slightly more males having risk than females having risk
"""

df.iloc[:,:5]

"""## Understanding Correlation"""

# To understand correlation between some features, pairplot is used
plt.figure(figsize=(20,15))
sns.pairplot(df.loc[:,'totChol': 'glucose'])

plt.figure(figsize=(15,15))
sns.heatmap(df.corr(), annot=True, linewidths=0.1)

"""Observations:
* From pairplot and heatmap we see that sysBP and diaBP are highly correlated
* And currentSmoker and cigsPerDay are highly correlated
"""

# dropping features which are highly correlated
# features_to_drop = ['currentSmoker', 'diaBP']

# df.drop(features_to_drop, axis=1, inplace=True)

"""# Data Cleaning and Preparation"""

# education feature is not required as its not predicting the Ten Year CHD
# target is Ten Year CHD (0 or 1)
# df.drop('education', axis=1, inplace=True)

# renaming TenYearCHD to CHD
df.rename(columns={"TenYearCHD": "CHD"}, inplace=True)

# Check 5 rows of data set
df.head()

"""## Scaling data

### Checking the Scale
"""

# Normalization Checking
fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)
ax = ax.flatten()

i = 0
for k,v in df.items():
    sns.displot(v, ax=ax[i])
    i+=1
    if i==12:
        break
plt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)

# Normalization Checking
fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)
ax = ax.flatten()

i = 0
for k,v in df.items():
    sns.histplot(v, ax=ax[i])
    i+=1
    if i==12:
        break
plt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)

"""### Obtaining Discrete and Continous Columns"""

# check for column that is not discrete to scale data

#all-columns list
df_all_colum=list(df.columns)

# discrete/categorical column list
discrete_cols=list(df.columns[df.round(decimals=0).isin([0,1]).all()])
discrete_cols.append('education')
discrete_cols.append('BPMeds')

#continuous column list
continuous_cols=list()
for i in df_all_colum:
    if i not in discrete_cols:
        if i not in['education', 'BPMeds']:
            continuous_cols.append(i)

#check if the two list are correct
print('Discrete cols: ', discrete_cols)

print('Continuous cols:', continuous_cols)

"""### MinMaxScaler"""

from sklearn.preprocessing import MinMaxScaler 
scaler = MinMaxScaler()

#scale continuous columns data
array_scaled = scaler.fit_transform(df[continuous_cols])

#turn array scaled to dataframe 
df_scaled_cols = pd.DataFrame(array_scaled, columns = [df[continuous_cols]])

#create a complete scaled data
df_scaled = df[df_all_colum]
df_scaled[continuous_cols] = df_scaled_cols[continuous_cols]

#datafram before scaling
df[continuous_cols].hist(bins=30, 
                         figsize=(20,40),
                         layout=(15,4)
                        );

#datafram after scaling
df_scaled[continuous_cols].hist(bins=30, 
                                figsize=(20,40),
                                layout=(15,4)
                                );

"""### StandardScaler"""

# # Standardise some features
# from sklearn.preprocessing import StandardScaler
# scaler = StandardScaler()
# cols_to_standardise = ['age','totChol','sysBP','BMI', 'heartRate', 'glucose', 'cigsPerDay']
# train_data[cols_to_standardise] = scaler.fit_transform(train_data[cols_to_standardise])

# #datafram before scaling
# df[continuous_cols].hist(bins=30, 
#                          figsize=(20,40),
#                          layout=(15,4)
#                         );

# #datafram after scaling
# df_scaled[continuous_cols].hist(bins=30, 
#                                 figsize=(20,40),
#                                 layout=(15,4)
#                                 );

"""## Handling duplicate data """

df_scaled.drop_duplicates()

"""## Handling missing data
Missing values can be done before EDA or after EDA. But before EDA, it will impute or drop missing values for all features, whether some features are needed or not
"""

!pip install missingno

df_scaled.isna().sum()

#check if there are any null value in dataset 
df_scaled.isnull().sum()

df_scaled.isnull().sum().sum()

missing_values_count = df_scaled.isnull().sum() 
missing_values_count = missing_values_count[missing_values_count > 0]
missing_values_percent = (missing_values_count * 100) / (df_scaled.shape[0])
print("The Percentage of Missing Values for the Columns are:")
print(missing_values_percent)

"""Percent Missing Values :

1.62% trestbps
60.9% fbs
0.81% restecg
0.81% thalach
0.81% exang
4.87% oldpeak
13.8% slope
95.9% ca
42.2% thal
We'll Fill NAN's of all features with median values of that particular feature because mean / average filling values approach won't make any sense since we've discrete values in some features.

One can also try Backward / Forward fill method.

We can observe that fbs and ca has 60.9% and 95.9% missing values respectively so we can drop these features.

Theoretically, 25 to 30% is the maximum missing values are allowed, beyond which we might want to drop the variable from analysis.
"""

print("Maximum missing percentage is {} and hence Imputation is required.".format(max(missing_values_percent)))

#see if there is any pattern in missing values 
import missingno as msno
msno.matrix(df_scaled)

'''
the visualization(matrix) does not show any obvious pattern or cluster in the missing 
values, hence, we assume that our data is missing completely at random(MCAR)
'''

#see if there is any correlation between missing values 
msno.heatmap(df_scaled, figsize=(20, 5))
'''
There seem to be an insignificant correlation between our missing value in colum(tochol)
and column(heartRate). Thus, this reinforce our assumption that our missing value is missing completely 
at random (MCAR)
'''

"""### SimpleImputer"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')

new_train_data_simple_imputer = pd.DataFrame(imputer.fit_transform(df_scaled))
new_train_data_simple_imputer.columns = df_scaled.columns
new_train_data_simple_imputer.index = df_scaled.index

#visualize to see if there are still missing data
msno.matrix(new_train_data_simple_imputer)

# Data before Imputation
df_scaled.isnull().sum()

# Data after Imputation
new_train_data_simple_imputer.isnull().sum()

new_train_data_simple_imputer.head()

# df_scaled_simple = new_train_data_simple_imputer.copy()

"""### IterativeImputer"""

#As we confirm that our missing value is MCAR, we will use iterative imputing to fill in NA value
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

#iterative imputer module, impute on the scaled dataset
imputer = IterativeImputer()
df_impute = imputer.fit_transform(df_scaled)

#impute dataframe 
df_impute = pd.DataFrame(data=df_impute, columns=df.columns)

#double check for missing value
df_impute.isnull().values.any()

#visualize to see if there are still missing data
msno.matrix(df_impute)

#after imputing, sometime data isn't rounded correctly
#BPMeds  
df_impute['BPMeds'] = df_impute['BPMeds'].round(decimals=0) 
df_impute['BPMeds'].value_counts()
#education  
# df_impute['education'] = df_impute['education'].round(decimals=0) 
# df_impute['education'].value_counts()

"""## Handling ouliers"""

from sklearn.model_selection import train_test_split

#Separate label (y) and predictor (X)
X = df_impute.drop('CHD', axis=1)
y = df_impute['CHD']

#Split the data set for winsorization (onliy winsorize on the train set and NOT on the test set)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

#check 
print('X_train:',X_train.shape,', X_test:', X_test.shape)

fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)
ax = ax.flatten()

i = 0
for k,v in df_impute.items():
    sns.boxplot(y=v, ax=ax[i])
    i+=1
    if i==12:
        break
plt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)

#visualize outlier (before winsorization)
plt.rcParams["figure.figsize"] = [17, 8]
plt.rcParams["figure.autolayout"] = True
plt.boxplot(x=df_impute[continuous_cols]);
plt.xticks(ticks=range(1,9), labels=np.array(continuous_cols));

"""Conclusion of Boxplot :

Outliers found in features named ['totChol', 'sysBP', 'BMI','heartRate', 'glucose']
"""

# for i in df.columns: 
#     sns.countplot(df[i], hue = df['TenYearCHD'], palette = 'Set1')
#     plt.show()

from scipy.stats.mstats import winsorize
from scipy.stats import mstats

# df_winzore = df_impute
#Winzorize the 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose' , 'cigPerDay' columns
totChol_wins = mstats.winsorize(X_train['totChol'], limits=[0.02, 0.009])
sysBP_wins = mstats.winsorize(X_train['sysBP'], limits=[0.02, 0.03])
diaBP_wins = mstats.winsorize(X_train['diaBP'], limits=[0.02, 0.01])
BMI_wins = mstats.winsorize(X_train['BMI'], limits=[0.02, 0.01])
heartRate_wins = mstats.winsorize(X_train['heartRate'], limits=[0.03, 0.01])
glucose_wins = mstats.winsorize(X_train['glucose'], limits=[0.02, 0.02])
cigsPerDay_wins = mstats.winsorize(X_train['cigsPerDay'], limits=[0.02, 0.01])

# Add the winsorized 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'Glucose' columns back to the train DataFrame
X_train['totChol'] = totChol_wins
X_train['sysBP'] = sysBP_wins
X_train['diaBP'] = diaBP_wins
X_train['BMI'] = BMI_wins
X_train['heartRate'] = heartRate_wins
X_train['glucose'] =glucose_wins
X_train['cigsPerDay'] =cigsPerDay_wins

# visualize outlier (after winsorization)
plt.boxplot(x=X_train[continuous_cols]);
plt.xticks(ticks=range(1,9), labels=np.array(continuous_cols));

# oulier distribution before winsorization
df_impute[continuous_cols].hist(bins=30, 
               figsize=(20,30),
               layout=(15,4));

# oulier distribution after winsorization
X_train[continuous_cols].hist(bins=30, 
               figsize=(20,30),
               layout=(15,4));

"""## Handling imbalanced data"""

!pip install imblearn

#SMOTE-tomek (SMOTE-Tomek is a combination of oversampling (SMOTE) and undersampling (Tomek links) techniques)
from imblearn.combine import SMOTETomek

#Create the SMOTE-tomek variable 
smote_tomek = SMOTETomek(random_state=42)

#Applie SMOTE-tomek to train data
X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)

#Original train dataset 
X_train.count(), y_train.value_counts()

#visualize original imbalanced data
y_train.hist(figsize=(20,10))

#Resample train dataset 
X_train_resampled.count(), y_train_resampled.value_counts()

#visualize balanced data after SMOTE-tomek
y_train_resampled.hist(figsize=(20,10))

"""# Feature Selection"""

import seaborn as sns 

#visualize features
corrmat= X_train_resampled.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,10))
g=sns.heatmap(X_train_resampled[top_corr_features].corr(),annot=True, cmap='RdYlGn')

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# remove label column from bianry(categorical) list
discrete_cols.remove('CHD')

#calculating chi2 score for CATEGORICAL variables
bestfeature = SelectKBest(score_func=chi2, k=7)
fit = bestfeature.fit(X_train_resampled[discrete_cols], y_train_resampled)

# Pass the result into a dataframe
dfp_values = pd.DataFrame(fit.pvalues_)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X_train_resampled[discrete_cols].columns)
featureScores = pd.concat([dfcolumns,dfscores,dfp_values],axis=1)
featureScores.columns = ['Specs','Score','p-value']  
featureScores_largest = featureScores.nlargest(7,'Score') 
print(featureScores.nlargest(7,'Score'))

#my feature list
feature_list = list(X_train_resampled.columns)
feature_list.remove('education') 
feature_list.remove('currentSmoker')
feature_list.remove('glucose')

feature_list

import seaborn as sns 
# visulize top features 
plt.figure(figsize=(20,15))
g=sns.heatmap(X_train_resampled[feature_list].corr(),annot=True, cmap='RdYlGn')

"""# Model Fitting

## Logistic Regression
"""

X_train_resampled.shape

"""### Importing Model Library"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

"""### Setting Class Weights"""

# Class Weighting (more weight on Minority Class - Class 1)
class_weight_LR = {0: 1, 1: 1.2}

"""### Normal Parameter Training

#### Creating Model Function
"""

# Create LogisticRegression Model
Model_LogisticRegression_Normal = LogisticRegression(max_iter=1000, solver='liblinear', class_weight=class_weight_LR)

"""#### Fitting Model"""

Model_LogisticRegression_Normal.fit(X_train, y_train)

"""#### Predicting"""

y_train_proba_LR_normal = Model_LogisticRegression_Normal.predict_proba(X_train)
y_test_proba_LR_normal = Model_LogisticRegression_Normal.predict_proba(X_test)

y_train_pred_LR_normal = Model_LogisticRegression_Normal.predict(X_train)
y_test_pred_LR_normal = Model_LogisticRegression_Normal.predict(X_test)

"""#### Model Evaluations"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion Matrix"""

confusion_matrix(y_test, y_test_pred_LR_normal)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test, y_test_pred_LR_normal) 
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_pred_LR_normal))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_pred_LR_normal))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_pred_LR_normal))
print('Testing Precision: ', precision_score(y_test, y_test_pred_LR_normal))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_pred_LR_normal))
print('Testing Recall: ', recall_score(y_test, y_test_pred_LR_normal))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_pred_LR_normal))
print('Testing F1-Score: ', f1_score(y_test, y_test_pred_LR_normal))

"""#### Model Results

##### Classification Report (Precision, Recall & F1 Score)
"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_LR = classification_report(y_test, y_test_pred_LR_normal, output_dict=True)
pd.DataFrame(classification_report_LR)

"""##### ROC Curve"""

#Roc curve
#pass value in to roc_curve variable: fpr, tpr, thresholds
fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_test_pred_LR_normal)
roc_auc_LR = auc(fpr_LR, tpr_LR)

#plot it
plt.figure(figsize=(10,5))
plt.plot(fpr_LR, tpr_LR, lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc_LR)
plt.plot([0, 1], [0, 1], '--', color='gray', lw=2)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Import roc_curve, auc
from sklearn.metrics import roc_curve, auc

# Calculate the probability scores of each point in the training set
y_train_score_LR_Normal = Model_LogisticRegression_Normal.decision_function(X_train)

# Calculate the fpr, tpr, and thresholds for the training set
train_fpr_LR_Normal, train_tpr_LR_Normal, thresholds_LR_Normal = roc_curve(y_train, y_train_score_LR_Normal)

# Calculate the probability scores of each point in the test set
y_test_score_LR_Normal = Model_LogisticRegression_Normal.decision_function(X_test)

# Calculate the fpr, tpr, and thresholds for the test set
test_fpr_LR_Normal, test_tpr_LR_Normal, test_thresholds_LR_Normal = roc_curve(y_test, y_test_score_LR_Normal)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Seaborn's beautiful styling
sns.set_style('darkgrid', {'axes.facecolor': '0.9'})

# ROC curve for training set
plt.figure(figsize=(10, 8))
lw = 2
plt.plot(train_fpr_LR_Normal, train_tpr_LR_Normal, color='darkorange', lw=lw, label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.yticks([i/20.0 for i in range(21)])
plt.xticks([i/20.0 for i in range(21)])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) Curve for Training Set')
plt.legend(loc='lower right')
print('Training AUC: {}'.format(auc(train_fpr_LR_Normal, train_tpr_LR_Normal)))
plt.show()

# ROC curve for test set
plt.figure(figsize=(10, 8))
lw = 2
plt.plot(test_fpr_LR_Normal, test_tpr_LR_Normal, color='darkorange', lw=lw, label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.yticks([i/20.0 for i in range(21)])
plt.xticks([i/20.0 for i in range(21)])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) Curve for Test Set')
plt.legend(loc='lower right')
print('Test AUC: {}'.format(auc(test_fpr_LR_Normal, test_tpr_LR_Normal)))
print('')
plt.show()

fpr, tpr, _ = roc_curve(y_train, y_train_proba_LR_normal[:,1])

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_test_proba_LR_normal[:,1])

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""##### ROC AUC (On Training Data)"""

from sklearn.metrics import roc_auc_score

roc_auc_score(y_train, y_train_proba_LR_normal[:,1])

"""##### ROC AUC (On Testing Data)"""

roc_auc_score(y_test, y_test_proba_LR_normal[:,1])

"""### Tuned Parameter Training

#### Defining Tunning Parameters
"""

# Defining Tunning Parameters
Hyperparameters_LR = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}

"""#### Creating GridSearchCV Model"""

# Creating GridSearchCV Model
Model_GridSearch_LR_Tuned = GridSearchCV(Model_LogisticRegression_Normal, Hyperparameters_LR, cv = 5, scoring = 'roc_auc')

"""#### Fitting data into GridSearchCV"""

# Fitting data into GridSearchCV
Model_GridSearch_LR_Tuned.fit(X_train_resampled[feature_list] , y_train_resampled)

"""#### Obtaining best Hyperparameters results and Model Score"""

# Obtaining best Hyperparameters results and Model Score
print(Model_GridSearch_LR_Tuned.best_params_, Model_GridSearch_LR_Tuned.best_score_)

# Model_GridSearch_LR_Tuned_Fit = Model_GridSearch_LR_Tuned.fit(X_train_resampled[feature_list] , y_train_resampled)

"""#### Obtaining Model Coefficients"""

# Model Coefficients
Model_GridSearch_LR_Tuned.best_estimator_.coef_

"""#### Model Intercepts"""

# Model Intercepts
Model_GridSearch_LR_Tuned.best_estimator_.intercept_

"""#### Predicting"""

y_train_proba_LR_Tuned = Model_GridSearch_LR_Tuned.predict_proba(X_train[feature_list])[:,1]
y_test_proba_LR_Tuned = Model_GridSearch_LR_Tuned.predict_proba(X_test[feature_list])[:,1]

y_train_predict_LR_Tuned = Model_GridSearch_LR_Tuned.predict(X_train[feature_list])
y_test_predict_LR_Tuned = Model_GridSearch_LR_Tuned.predict(X_test[feature_list])

"""#### Checking Threshold Value"""

# Threshold value can be adjusted to prioritize certain types of errors over others.
# ex: customize threshold to increase sensitivity  to patience with the disease (reduce false negative)
y_pred_test_LR_Tuned = (y_test_proba_LR_Tuned > 0.5).astype(int)

"""#### Model Evaluations"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion matrix """

confusion_matrix(y_test, y_test_predict_LR_Tuned)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test, y_test_predict_LR_Tuned) 
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_predict_LR_Tuned))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_predict_LR_Tuned))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_predict_LR_Tuned))
print('Testing Precision: ', precision_score(y_test, y_test_predict_LR_Tuned))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_predict_LR_Tuned))
print('Testing Recall: ', recall_score(y_test, y_test_predict_LR_Tuned))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_predict_LR_Tuned))
print('Testing F1-Score: ', f1_score(y_test, y_test_predict_LR_Tuned))

"""#### Model Results

##### Classification Report (Precision, Recall & F1 Score)
"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_LR = classification_report(y_test, y_test_predict_LR_Tuned, output_dict=True)
pd.DataFrame(classification_report_LR)

"""##### ROC Curve"""

#Roc curve
#pass value in to roc_curve variable: fpr, tpr, thresholds
fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_test_predict_LR_Tuned)
roc_auc_LR = auc(fpr_LR, tpr_LR)

#plot it
plt.figure(figsize=(10,5))
plt.plot(fpr_LR, tpr_LR, lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc_LR)
plt.plot([0, 1], [0, 1], '--', color='gray', lw=2)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""##### ROC AUC (On Training Data)"""

roc_auc_score(y_train, y_train_proba_LR_Tuned)

"""##### ROC AUC (On Testing Data)"""

roc_auc_score(y_test, y_test_proba_LR_Tuned)

"""## Decision Tree Classifier

### Importing Model Library
"""

from sklearn.tree import DecisionTreeClassifier

"""### Normal Parameter Tuning

#### Creating Model Function
"""

Model_DecisionTreeClassifier_Normal = DecisionTreeClassifier(min_samples_split=40, random_state=0) 
# that fraction of samples(if float) or that many number(if int) of samples is atleast present in the node 
# before splitting, then only split that node

# for min_samples_split as 180 I got a better accuracy and train score and difference was less
# but f1 score was very bad for positive class
# and setting min_samples_split as 40, we got good results for all metrics

"""#### Fitting Model"""

Model_DecisionTreeClassifier_Normal.fit(X_train, y_train)

"""#### Predicting"""

y_train_proba_DT_normal = Model_DecisionTreeClassifier_Normal.predict_proba(X_train)
y_test_proba_DT_normal = Model_DecisionTreeClassifier_Normal.predict_proba(X_test)

y_train_pred_DT_normal = Model_DecisionTreeClassifier_Normal.predict(X_train)
y_test_pred_DT_normal = Model_DecisionTreeClassifier_Normal.predict(X_test)

"""#### Model Evaluations"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion Matrix"""

confusion_matrix(y_test, y_test_pred_DT_normal)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test, y_test_pred_DT_normal) 
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_pred_DT_normal))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_pred_DT_normal))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_pred_DT_normal))
print('Testing Precision: ', precision_score(y_test, y_test_pred_DT_normal))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_pred_DT_normal))
print('Testing Recall: ', recall_score(y_test, y_test_pred_DT_normal))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_pred_DT_normal))
print('Testing F1-Score: ', f1_score(y_test, y_test_pred_DT_normal))

"""#### Modal Results

##### Classification Report (Precision, Recall & F1 Score)
"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_LR = classification_report(y_test, y_test_pred_DT_normal, output_dict=True)
pd.DataFrame(classification_report_LR)

from sklearn.metrics import roc_curve, roc_auc_score

"""##### ROC AUC (On Training Data)"""

roc_auc_score(y_train, y_train_proba_DT_normal[:,1])

"""##### ROC AUC (On Testing Data)"""

roc_auc_score(y_test, y_test_proba_DT_normal[:,1])

"""##### ROC Curve"""

#Roc curve
#pass value in to roc_curve variable: fpr, tpr, thresholds
fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_test_pred_DT_normal)
roc_auc_LR = auc(fpr_LR, tpr_LR)

#plot it
plt.figure(figsize=(10,5))
plt.plot(fpr_LR, tpr_LR, lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc_LR)
plt.plot([0, 1], [0, 1], '--', color='gray', lw=2)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

fpr, tpr, _ = roc_curve(y_train, y_train_proba_DT_normal[:,1])

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_test_proba_DT_normal[:,1])

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""#### Obtaining the Decision Tree"""

# Exporting the tree in text format
from sklearn.tree import export_text
text_format_DT = export_text(Model_DecisionTreeClassifier_Normal, feature_names = list(df_impute.columns[:15]))
print('Decision tree in text format : \n%s'%text_format_DT)

from sklearn import tree
fig = plt.figure(figsize=(25,20))
_ = tree.plot_tree(Model_DecisionTreeClassifier_Normal,
                   feature_names=X_train_resampled.columns,
                   class_names=['No Disease', "Disease"],
                   filled=True)

"""### Tuned Parameter Training

#### Defining Tunning Parameters
"""

hyperparameters_DT = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

"""#### Creating GridSearchCV Model"""

# Creating GridSearchCV Model
Model_GridSearch_DT_Tuned = GridSearchCV(Model_DecisionTreeClassifier_Normal, hyperparameters_DT, cv = 5, scoring = 'roc_auc')

"""#### Fitting data into GridSearchCV"""

# Fitting data into GridSearchCV
Model_GridSearch_DT_Tuned.fit(X_train_resampled[feature_list] , y_train_resampled)

"""#### Obtaining best Hyperparameters results and Model Score"""

# Obtaining best Hyperparameters results and Model Score
print(Model_GridSearch_DT_Tuned.best_params_, Model_GridSearch_DT_Tuned.best_score_)

best_estimator_DT = Model_GridSearch_DT_Tuned.best_estimator_
best_estimator_DT

"""#### Re-fitting Data into the Model"""

# Model_DecisionTreeClassifier_Refit.fit(X_train, y_train)

"""#### Obtaining Model Coefficients"""

# Model Coefficients
# Model_GridSearch_DT_Tuned.best_estimator_.coef_
Model_GridSearch_DT_Tuned.best_estimator_

"""#### Model Intercepts"""

# Model Intercepts
Model_GridSearch_LR_Tuned.best_estimator_.intercept_

"""#### Predicting"""

y_train_proba_DT_Tuned = Model_GridSearch_DT_Tuned.predict_proba(X_train[feature_list])[:,1]
y_test_proba_DT_Tuned = Model_GridSearch_DT_Tuned.predict_proba(X_test[feature_list])[:,1]

y_train_predict_DT_Tuned = Model_GridSearch_DT_Tuned.predict(X_train[feature_list])
y_test_predict_DT_Tuned = Model_GridSearch_DT_Tuned.predict(X_test[feature_list])

"""#### Model Evaluation"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

"""##### Confusion Matrix"""

confusion_matrix(y_test, y_test_predict_DT_Tuned)

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_test_predict_DT_Tuned) 
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_predict_DT_Tuned))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_predict_DT_Tuned))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_predict_DT_Tuned))
print('Testing Precision: ', precision_score(y_test, y_test_predict_DT_Tuned))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_predict_DT_Tuned))
print('Testing Recall: ', recall_score(y_test, y_test_predict_DT_Tuned))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_predict_DT_Tuned))
print('Testing F1-Score: ', f1_score(y_test, y_test_predict_DT_Tuned))

"""##### Accuracy Score"""

accuracy_DT = accuracy_score(y_test_predict_DT_Tuned, y_test)*100
print('Accuracy score for Decision tree is %f'%accuracy_DT)

"""##### Train Score"""

train_score_DT = Model_GridSearch_DT_Tuned.score(X_train_resampled[feature_list], y_train_resampled)*100
print('Train score for Decision tree is %f'%train_score_DT)

"""##### Difference between Training and Testing Score"""

print('Difference between train and test scores for Decision tree is : %f'%(train_score_DT - accuracy_DT))

"""#### Model Results"""

score_DT = pd.DataFrame(Model_GridSearch_DT_Tuned.cv_results_)
score_DT

"""##### Classification Report (Precision, Recall & F1 Score)"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_DT = classification_report(y_test, y_test_predict_DT_Tuned, output_dict=True)
pd.DataFrame(classification_report_DT)

"""##### ROC Curve"""

#Roc curve
#pass value in to roc_curve variable: fpr, tpr, thresholds
fpr_DT, tpr_DT, thresholds_DT = roc_curve(y_test, y_test_predict_DT_Tuned)
roc_auc_DT = auc(fpr_DT, tpr_DT)

#plot it
plt.figure(figsize=(10,5))
plt.plot(fpr_DT, tpr_DT, lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc_DT)
plt.plot([0, 1], [0, 1], '--', color='gray', lw=2)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

#Roc curve
#pass value in to roc_curve variable: fpr, tpr, thresholds
fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_test_predict_DT_Tuned)
roc_auc_LR = auc(fpr_LR, tpr_LR)

#plot it
plt.figure(figsize=(10,5))
plt.plot(fpr_LR, tpr_LR, lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc_LR)
plt.plot([0, 1], [0, 1], '--', color='gray', lw=2)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

fpr, tpr, _ = roc_curve(y_train, y_train_proba_DT_Tuned)

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_test_proba_DT_Tuned)

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""##### Confusion Matrix"""

confusion_matrix(y_test_predict_DT_Tuned, y_test)

"""##### Classification Report"""

print(classification_report(y_test_predict_DT_Tuned, y_test))

"""## Random Forest Classifier

### Importing Model Library
"""

from sklearn.ensemble import RandomForestClassifier

"""### Normal Parameter Training

#### Creating Model Function
"""

Model_RandomForestClassifier_Normal = RandomForestClassifier(n_estimators = 150, min_samples_split = 10, random_state = 0)

"""#### Fitting Model"""

Model_RandomForestClassifier_Normal.fit(X_train, y_train)

"""#### Predicting"""

y_train_proba_RF_normal = Model_RandomForestClassifier_Normal.predict_proba(X_train)
y_test_proba_RF_normal = Model_RandomForestClassifier_Normal.predict_proba(X_test)

y_train_pred_RF_normal = Model_RandomForestClassifier_Normal.predict(X_train)
y_test_pred_RF_normal = Model_RandomForestClassifier_Normal.predict(X_test)

"""#### Model Evaluations"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion Matrix"""

confusion_matrix(y_test, y_test_pred_RF_normal)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test, y_test_pred_RF_normal)
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_pred_RF_normal))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_pred_RF_normal))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_pred_RF_normal))
print('Testing Recall: ', recall_score(y_test, y_test_pred_RF_normal))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_pred_RF_normal))
print('Testing Precision: ', precision_score(y_test, y_test_pred_RF_normal))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_pred_RF_normal))
print('Testing F1-Score: ', f1_score(y_test, y_test_pred_RF_normal))

"""#### Model Results

##### Classification Report (Precision, Recall & F1 Score)
"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_DT = classification_report(y_test, y_test_predict_DT_Tuned, output_dict=True)
pd.DataFrame(classification_report_DT)

"""##### ROC Curve"""

roc_auc_score(y_train, y_train_proba_RF_normal[:,1])

roc_auc_score(y_test, y_test_proba_RF_normal[:,1])

fpr, tpr, _ = roc_curve(y_train, y_train_proba_RF_normal[:,1])

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_test_proba_RF_normal[:,1])

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""### Tuned Parameter Training

#### Defining Tuning Parameters
"""

n_estimators = [5,20,50,100]                                    # number of trees in the random forest
max_features = ['sqrt']                                         # number of features in consideration at every split ('auto' is deprecated)
max_depth    = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree
min_samples_split = [2, 6, 10]                                  # minimum sample number to split a node
min_samples_leaf = [1, 3, 4]                                    # minimum sample number that can be stored in a leaf node
bootstrap = [True, False]                                       # method used to sample data points

hyperparameters_RF = {'n_estimators': n_estimators,
                      'max_features': max_features,
                      'max_depth': max_depth,
                      'min_samples_split': min_samples_split,
                      'min_samples_leaf': min_samples_leaf,
                      'bootstrap': bootstrap
                      }

"""#### Creating GridSearchCV Model"""

# Creating GridSearchCV Model
Model_GridSearchCV_RF_Tuned = GridSearchCV(Model_RandomForestClassifier_Normal, hyperparameters_RF, cv = 5, scoring = 'roc_auc')

"""#### Creating RandomizedSearchCV Model"""

from sklearn.model_selection import RandomizedSearchCV
Model_RandomizedSearchCV_RF_Tuned = RandomizedSearchCV(estimator = Model_RandomForestClassifier_Normal,
                                      param_distributions = hyperparameters_RF,
                                      n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)

Model_RandomizedSearchCV_RF_Tuned

"""#### Fitting data into RandomizedSearchCV"""

Model_RandomizedSearchCV_RF_Tuned.fit(X_train_resampled[feature_list] , y_train_resampled)

# Fitting data into GridSearchCV
# WAY TOO TIME CONSUMING
# Model_GridSearchCV_RF_Tuned.fit(X_train_resampled[feature_list] , y_train_resampled),

"""#### Obtaining best Hyperparameters results and Model Score"""

print(Model_RandomizedSearchCV_RF_Tuned.best_estimator_)

# Obtaining best Hyperparameters results and Model Score
print(Model_RandomizedSearchCV_RF_Tuned.best_params_, Model_RandomizedSearchCV_RF_Tuned.best_score_)

print(Model_RandomizedSearchCV_RF_Tuned.cv_results_)

best_estimator_RF = Model_RandomizedSearchCV_RF_Tuned.best_estimator_
best_estimator_RF

"""#### Predicting"""

y_train_proba_RF_Tuned = Model_RandomizedSearchCV_RF_Tuned.predict_proba(X_train[feature_list])[:,1]
y_test_proba_RF_Tuned = Model_RandomizedSearchCV_RF_Tuned.predict_proba(X_test[feature_list])[:,1]

y_train_predict_RF_Tuned = Model_RandomizedSearchCV_RF_Tuned.predict(X_train[feature_list])
y_test_predict_RF_Tuned = Model_RandomizedSearchCV_RF_Tuned.predict(X_test[feature_list])

"""#### Checking Threshold Value"""

# Threshold value can be adjusted to prioritize certain types of errors over others.
# ex: customize threshold to increase sensitivity  to patience with the disease (reduce false negative)
# y_pred_test_LR_Tuned = (y_test_proba_RF_Tuned > 0.5).astype(int)

"""#### Obtaining Model Coefficients"""

# Model Coefficients
# Model_RandomizedSearchCV_RF_Tuned.best_estimator_.coef_
Model_RandomizedSearchCV_RF_Tuned.best_estimator_

"""#### Model Intercepts"""

# Model Intercepts
# Model_RandomizedSearchCV_RF_Tuned.best_estimator_.intercept_

pd.DataFrame(y_test_proba_RF_Tuned)

"""#### Fitting Data into the Model"""

# Model_RandomizedSearchCV_RF_Tuned.fit(X_train, y_train)

"""#### Model Evaluation

##### Accuracy Score
"""

accuracy_RF = accuracy_score(y_test_predict_RF_Tuned, y_test)*100
print('Accuracy score for Random Forest is %f'%accuracy_RF)

"""##### Training Score"""

train_score_RF = Model_RandomizedSearchCV_RF_Tuned.score(X_train[feature_list], y_train)*100
print('Train score for Random Forest is %f'%train_score_RF)

"""##### Difference between Training and Testing Score"""

print('Difference between train and test scores for Random Forest is : %f'%(train_score_RF - accuracy_RF))

"""#### Model Results"""

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion Matrix"""

confusion_matrix(y_test_predict_RF_Tuned, y_test)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test_predict_RF_Tuned, y_test)
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_predict_RF_Tuned))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_predict_RF_Tuned))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_predict_RF_Tuned))
print('Testing Precision: ', precision_score(y_test, y_test_predict_RF_Tuned))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_predict_RF_Tuned))
print('Testing Recall: ', recall_score(y_test, y_test_predict_RF_Tuned))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_predict_RF_Tuned))
print('Testing F1-Score: ', f1_score(y_test, y_test_predict_RF_Tuned))

"""##### Classification Report"""

print(classification_report(y_test_predict_RF_Tuned, y_test))

"""##### ROC Curve"""

roc_auc_score(y_train, y_train_proba_RF_Tuned)

roc_auc_score(y_test, y_test_proba_RF_Tuned)

fpr, tpr, _ = roc_curve(y_train, y_train_proba_RF_Tuned)

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_test_proba_RF_Tuned)

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""## SVC

### Importing Model Library
"""

from sklearn.svm import SVC

from sklearn.model_selection import cross_val_score

"""### Normal Parameter Training

#### Creating Model Function
"""

Model_SVC_Normal = SVC(random_state=42, probability=True)

"""#### Fitting Model"""

Model_SVC_Normal.fit(X_train, y_train)

"""#### Predicting"""

y_train_pred_SVC_normal = Model_SVC_Normal.predict(X_train)
y_test_pred_SVC_normal = Model_SVC_Normal.predict(X_test)

y_train_proba_SVC_normal = Model_SVC_Normal.predict_proba(X_train)
y_test_proba_SVC_normal = Model_SVC_Normal.predict_proba(X_test)

"""#### Model Evaluations"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion Matrix"""

confusion_matrix(y_test, y_test_pred_SVC_normal)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test, y_test_pred_SVC_normal)
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_pred_SVC_normal))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_pred_SVC_normal))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_pred_SVC_normal))
print('Testing Precision: ', precision_score(y_test, y_test_pred_SVC_normal))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_pred_SVC_normal))
print('Testing Recall: ', recall_score(y_test, y_test_pred_SVC_normal))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_pred_SVC_normal))
print('Testing F1-Score: ', f1_score(y_test, y_test_pred_SVC_normal))

"""#### Modal Results

##### Classification Report
"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_SVC = classification_report(y_test, y_test_pred_SVC_normal, output_dict=True)
pd.DataFrame(classification_report_SVC)

"""##### ROC Curve"""

# Import roc_curve, auc
from sklearn.metrics import roc_curve, auc

# Calculate the probability scores of each point in the training set
y_train_score_SVC_Normal = Model_SVC_Normal.decision_function(X_train)

# Calculate the fpr, tpr, and thresholds for the training set
train_fpr_SVC_Normal, train_tpr_SVC_Normal, thresholds_SVC_Normal = roc_curve(y_train, y_train_score_SVC_Normal)

# Calculate the probability scores of each point in the test set
y_test_score_SVC_Normal = Model_SVC_Normal.decision_function(X_test)

# Calculate the fpr, tpr, and thresholds for the test set
test_fpr_SVC_Normal, test_tpr_SVC_Normal, test_thresholds_SVC_Normal = roc_curve(y_test, y_test_score_SVC_Normal)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Seaborn's beautiful styling
sns.set_style('darkgrid', {'axes.facecolor': '0.9'})

# ROC curve for training set
plt.figure(figsize=(10, 8))
lw = 2
plt.plot(train_fpr_SVC_Normal, train_tpr_SVC_Normal, color='darkorange', lw=lw, label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.yticks([i/20.0 for i in range(21)])
plt.xticks([i/20.0 for i in range(21)])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) Curve for Training Set')
plt.legend(loc='lower right')
print('Training AUC: {}'.format(auc(train_fpr_SVC_Normal, train_tpr_SVC_Normal)))
plt.show()

# ROC curve for test set
plt.figure(figsize=(10, 8))
lw = 2
plt.plot(test_fpr_SVC_Normal, test_tpr_SVC_Normal, color='darkorange', lw=lw, label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.yticks([i/20.0 for i in range(21)])
plt.xticks([i/20.0 for i in range(21)])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) Curve for Test Set')
plt.legend(loc='lower right')
print('Test AUC: {}'.format(auc(test_fpr_SVC_Normal, test_tpr_SVC_Normal)))
print('')
plt.show()

roc_auc_score(y_train, y_train_proba_SVC_normal[:,1])

roc_auc_score(y_test, y_test_proba_SVC_normal[:,1])

fpr, tpr, _ = roc_curve(y_train, y_train_proba_RF_Tuned)

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_test_proba_RF_Tuned)

plt.clf()
plt.plot(fpr, tpr)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""### Tuned Parameter Training

#### Defining Tuning Parameters
"""

hyperparameters_SVC = { 
    'kernel': ['poly', 'rbf', 'sigmoid'],
    'degree': [2, 3, 4, 5, 6],
    'gamma' : ['scale', 'auto']
}

"""#### Creating GridSearchCV Model"""

# Model_GridSearchCV_SVC_Tuned = GridSearchCV(estimator = Model_SVC_Normal, param_grid = hyperparameters_SVC, cv = 10)

Model_RandomizedSearchCV_SVC_Tuned = RandomizedSearchCV(estimator = Model_SVC_Normal,
                                      param_distributions = hyperparameters_SVC,
                                      n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)

"""#### Fitting Data into GridSearchCV Model"""

Model_RandomizedSearchCV_SVC_Tuned.fit(X_train_resampled[feature_list] , y_train_resampled)

"""#### Obtaining Model Best Parameters"""

print(Model_RandomizedSearchCV_SVC_Tuned.best_params_)
print(Model_RandomizedSearchCV_SVC_Tuned.best_score_)

"""#### Predicting"""

y_train_proba_SVC_Tuned = Model_RandomizedSearchCV_SVC_Tuned.predict_proba(X_train[feature_list])[:,1]
y_test_proba_SVC_Tuned = Model_RandomizedSearchCV_SVC_Tuned.predict_proba(X_test[feature_list])[:,1]

y_train_predict_SVC_Tuned = Model_RandomizedSearchCV_SVC_Tuned.predict(X_train[feature_list])
y_test_predict_SVC_Tuned = Model_RandomizedSearchCV_SVC_Tuned.predict(X_test[feature_list])

"""#### Obtaining Model Coefficients"""

# Model Coefficients
Model_RandomizedSearchCV_SVC_Tuned.best_estimator_

if(Model_RandomizedSearchCV_SVC_Tuned.best_estimator_.kernel == 'linear'):
  print(Model_RandomizedSearchCV_SVC_Tuned.best_estimator_.coef_)
else:
  print("Cofficient is available only when kernel is linear.")

"""### Model Evaluation"""

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, auc

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

"""##### Confusion Matrix"""

confusion_matrix(y_test_predict_SVC_Tuned, y_test)

# Confusion matrix 
conf_matrix = confusion_matrix(y_test_predict_SVC_Tuned, y_test) 
plt.figure(figsize=(10,5))
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, fmt='.1%', cmap='Oranges', linewidths=5, annot_kws={"fontsize":16})

"""##### Accuracy"""

print('Training Accuracy: ', accuracy_score(y_train, y_train_predict_SVC_Tuned))
print('Testing Accuracy: ', accuracy_score(y_test, y_test_predict_SVC_Tuned))

"""##### Precision"""

print('Training Precision: ', precision_score(y_train, y_train_predict_SVC_Tuned))
print('Testing Precision: ', precision_score(y_test, y_test_predict_SVC_Tuned))

"""##### Recall"""

print('Training Recall: ', recall_score(y_train, y_train_predict_SVC_Tuned))
print('Testing Recall: ', recall_score(y_test, y_test_predict_SVC_Tuned))

"""##### F1-Score"""

print('Training F1-Score: ', f1_score(y_train, y_train_predict_SVC_Tuned))
print('Testing F1-Score: ', f1_score(y_test, y_test_predict_SVC_Tuned))

"""### Model Results

##### Classification Report (Precision, Recall & F1 Score)
"""

# Classification Report (Precision, Recall & F1 Score)
classification_report_SVC = classification_report(y_test, y_test_predict_SVC_Tuned, output_dict=True)
pd.DataFrame(classification_report_SVC)

"""##### ROC Curve"""

# Import roc_curve, auc
from sklearn.metrics import roc_curve, auc

# Calculate the probability scores of each point in the training set
y_train_score_SVC_Tuned = Model_RandomizedSearchCV_SVC_Tuned.decision_function(X_train[feature_list])

# Calculate the fpr, tpr, and thresholds for the training set
train_fpr_SVC_Tuned, train_tpr_SVC_Tuned, thresholds_SVC_Tuned = roc_curve(y_train, y_train_score_SVC_Tuned)

# Calculate the probability scores of each point in the test set
y_test_score_SVC_Tuned = Model_RandomizedSearchCV_SVC_Tuned.decision_function(X_test[feature_list])

# Calculate the fpr, tpr, and thresholds for the test set
test_fpr_SVC_Tuned, test_tpr_SVC_Tuned, test_thresholds_SVC_Tuned = roc_curve(y_test, y_test_score_SVC_Tuned)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Seaborn's beautiful styling
sns.set_style('darkgrid', {'axes.facecolor': '0.9'})

# ROC curve for training set
plt.figure(figsize=(10, 8))
lw = 2
plt.plot(train_fpr_SVC_Tuned, train_tpr_SVC_Tuned, color='darkorange', lw=lw, label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.yticks([i/20.0 for i in range(21)])
plt.xticks([i/20.0 for i in range(21)])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) Curve for Training Set')
plt.legend(loc='lower right')
print('Training AUC: {}'.format(auc(train_fpr_SVC_Tuned, train_tpr_SVC_Tuned)))
plt.show()

#Roc curve
#pass value in to roc_curve variable: fpr, tpr, thresholds
fpr_SVC, tpr_SVC, thresholds_SVC = roc_curve(y_test, y_test_predict_SVC_Tuned)
roc_auc_SVC = auc(fpr_SVC, tpr_SVC)

#plot it
plt.figure(figsize=(10,5))
plt.plot(fpr_SVC, tpr_SVC, lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc_SVC)
plt.plot([0, 1], [0, 1], '--', color='gray', lw=2)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""#### Cross Validation Score"""

Model_GridSearchCV = SVC(degree = 2, gamma = 'scale', kernel= 'poly')
print(cross_val_score(Model_RandomizedSearchCV_SVC_Tuned, X_train, y_train, cv = 10, scoring = 'accuracy'))

"""# Comparison of Results

Out of all the models taken into consideration, the Model with the highest accuracy is the Random Forest Classifier. Hence, this model would be downloaded and used in the interface.

# Saving the Model for User Interface
"""

#create a pickle file using serialization (for streamlit)
import pickle
pickle_out = open("/content/mydrive/MyDrive/Capstone Project/Heart Disease Prediction/Outputs/Saved Model/model.pk1", "wb")
pickle.dump(Model_RandomForestClassifier_Normal, pickle_out)
pickle_out.close()